{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Convolution2D\nfrom keras.layers import MaxPooling2D\nfrom keras.layers import Flatten\nfrom keras.layers import Dense","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Initialising the CNN\nclassifier = Sequential()\n\n# Step 1 - Convolution\nclassifier.add(Convolution2D(32, 3, 3, input_shape = (64, 64, 3), activation = 'relu'))\n\n# Step 2 - Pooling\nclassifier.add(MaxPooling2D(pool_size = (2, 2)))\n\n# Adding a second convolutional layer\nclassifier.add(Convolution2D(32, 3, 3, activation = 'relu'))\nclassifier.add(MaxPooling2D(pool_size = (2, 2)))\n\n# Step 3 - Flattening\nclassifier.add(Flatten())\n\n# Step 4 - Full connection\nclassifier.add(Dense(units = 128, activation = 'relu'))\nclassifier.add(Dense(units = 1, activation = 'sigmoid')) #softmax\n","execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy']) #categorical_crossentropy\n","execution_count":11,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"classifier.summary()","execution_count":12,"outputs":[{"output_type":"stream","text":"Model: \"sequential_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d_2 (Conv2D)            (None, 21, 21, 32)        896       \n_________________________________________________________________\nmax_pooling2d_2 (MaxPooling2 (None, 10, 10, 32)        0         \n_________________________________________________________________\nconv2d_3 (Conv2D)            (None, 3, 3, 32)          9248      \n_________________________________________________________________\nmax_pooling2d_3 (MaxPooling2 (None, 1, 1, 32)          0         \n_________________________________________________________________\nflatten_1 (Flatten)          (None, 32)                0         \n_________________________________________________________________\ndense_2 (Dense)              (None, 128)               4224      \n_________________________________________________________________\ndense_3 (Dense)              (None, 1)                 129       \n=================================================================\nTotal params: 14,497\nTrainable params: 14,497\nNon-trainable params: 0\n_________________________________________________________________\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing.image import ImageDataGenerator\n","execution_count":13,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_datagen = ImageDataGenerator(rescale = 1./255,\n                                   shear_range = 0.2,\n                                   zoom_range = 0.2,\n                                   horizontal_flip = True)\n\ntest_datagen = ImageDataGenerator(rescale = 1./255)","execution_count":14,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"training_set = train_datagen.flow_from_directory('../input/skin-cancer-malignant-vs-benign/train',\n                                                 target_size = (64, 64), # make sure to change target_size if you altered input_shape above\n                                                 batch_size = 32,\n                                                 class_mode = 'binary')\ntest_set = test_datagen.flow_from_directory('../input/skin-cancer-malignant-vs-benign/test',\n                                            target_size = (64, 64),\n                                            batch_size = 32,\n                                            class_mode = 'binary')","execution_count":15,"outputs":[{"output_type":"stream","text":"Found 2637 images belonging to 2 classes.\nFound 660 images belonging to 2 classes.\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"classifier.fit_generator(training_set,\n                         steps_per_epoch = 64,\n                         epochs = 25,\n                         validation_data = test_set,\n                         validation_steps = 256)","execution_count":21,"outputs":[{"output_type":"stream","text":"Epoch 1/25\n64/64 [==============================] - 9s 147ms/step - loss: 0.4574 - accuracy: 0.7753 - val_loss: 0.4573 - val_accuracy: 0.7909\nEpoch 2/25\n64/64 [==============================] - 7s 116ms/step - loss: 0.4267 - accuracy: 0.7881\nEpoch 3/25\n64/64 [==============================] - 7s 107ms/step - loss: 0.4251 - accuracy: 0.7896\nEpoch 4/25\n64/64 [==============================] - 7s 111ms/step - loss: 0.4220 - accuracy: 0.7988\nEpoch 5/25\n64/64 [==============================] - 9s 133ms/step - loss: 0.4170 - accuracy: 0.7999\nEpoch 6/25\n64/64 [==============================] - 7s 115ms/step - loss: 0.4202 - accuracy: 0.7851\nEpoch 7/25\n64/64 [==============================] - 7s 109ms/step - loss: 0.3970 - accuracy: 0.8091\nEpoch 8/25\n64/64 [==============================] - 7s 114ms/step - loss: 0.4017 - accuracy: 0.8013\nEpoch 9/25\n64/64 [==============================] - 8s 119ms/step - loss: 0.4069 - accuracy: 0.8038\nEpoch 10/25\n64/64 [==============================] - 7s 110ms/step - loss: 0.4117 - accuracy: 0.7993\nEpoch 11/25\n64/64 [==============================] - 8s 118ms/step - loss: 0.4052 - accuracy: 0.8063\nEpoch 12/25\n64/64 [==============================] - 7s 115ms/step - loss: 0.3770 - accuracy: 0.8213\nEpoch 13/25\n64/64 [==============================] - 8s 127ms/step - loss: 0.3876 - accuracy: 0.8083\nEpoch 14/25\n64/64 [==============================] - 7s 115ms/step - loss: 0.4019 - accuracy: 0.8083\nEpoch 15/25\n64/64 [==============================] - 7s 116ms/step - loss: 0.3710 - accuracy: 0.8196\nEpoch 16/25\n64/64 [==============================] - 7s 108ms/step - loss: 0.3815 - accuracy: 0.8255\nEpoch 17/25\n64/64 [==============================] - 8s 128ms/step - loss: 0.3856 - accuracy: 0.8140\nEpoch 18/25\n64/64 [==============================] - 7s 116ms/step - loss: 0.3957 - accuracy: 0.8034\nEpoch 19/25\n64/64 [==============================] - 7s 111ms/step - loss: 0.3722 - accuracy: 0.8241\nEpoch 20/25\n64/64 [==============================] - 7s 105ms/step - loss: 0.3672 - accuracy: 0.8255\nEpoch 21/25\n64/64 [==============================] - 8s 131ms/step - loss: 0.3893 - accuracy: 0.8142\nEpoch 22/25\n64/64 [==============================] - 7s 110ms/step - loss: 0.3727 - accuracy: 0.8206\nEpoch 23/25\n64/64 [==============================] - 7s 102ms/step - loss: 0.3883 - accuracy: 0.8063\nEpoch 24/25\n64/64 [==============================] - 7s 106ms/step - loss: 0.3670 - accuracy: 0.8236\nEpoch 25/25\n64/64 [==============================] - 7s 103ms/step - loss: 0.3528 - accuracy: 0.8354\n","name":"stdout"},{"output_type":"execute_result","execution_count":21,"data":{"text/plain":"<tensorflow.python.keras.callbacks.History at 0x7fee9030c610>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# saving model to JSON\nmodel_json = classifier.to_json()\nwith open(\"model.json\", \"w\") as json_file:\n    json_file.write(model_json)\n# saving weights to HDF5\nclassifier.save_weights(\"model.h5\")","execution_count":22,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Making prediction about user data\nfrom keras.models import model_from_json\nfrom keras.preprocessing.image import ImageDataGenerator","execution_count":23,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# load json and create model\njson_file = open('./model.json', 'r')\nloaded_model_json = json_file.read()\njson_file.close()\nloaded_model = model_from_json(loaded_model_json)\n# load weights into new model\nloaded_model.load_weights(\"./model.h5\")","execution_count":35,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_datagen = ImageDataGenerator(rescale = 1./255)\nuser_data = test_datagen.flow_from_directory('../input/skin-cancer-malignant-vs-benign/test',\n                                            target_size = (64, 64),\n                                            batch_size = 1,\n                                            class_mode = 'binary')\n ","execution_count":36,"outputs":[{"output_type":"stream","text":"Found 660 images belonging to 2 classes.\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# evaluate loaded model on user data\nloaded_model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\nscore = loaded_model.evaluate(user_data)\nif score[1] == 0:\n    print('The convolutional neural network predicts that this tumor is malignant!')\nelse:\n    print('The convolutional neural network predicts that this tumor is benign.')\n","execution_count":37,"outputs":[{"output_type":"stream","text":"660/660 [==============================] - 2s 4ms/step - loss: 0.3291 - accuracy: 0.8470\nThe convolutional neural network predicts that this tumor is benign.\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}